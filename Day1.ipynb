{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devdc142196/python/blob/main/Day1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpmbKqI7jltH",
        "outputId": "0ec1bf68-5f68-4ec7-f746-ea081ec8cab5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world\n"
          ]
        }
      ],
      "source": [
        "print(\"hello world\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "# Create a Spark session\n",
        "spark =SparkSession.builder.appName(\"dev\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "(1,10),(2,20),(3,30),(4,40),(5,50)\n",
        "]\n",
        "columns=[\"id\",\"value\"]\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data,columns)\n",
        "# Show the DataFrame\n",
        "df.show()\n",
        "x=df.count()\n",
        "\n",
        "print(\"number of rows in above  dataframe is:\",x)\n",
        "\n",
        "#sum\n",
        "\n",
        "sm=df.select(sum(\"value\").alias(\"sum\"))\n",
        "sm.show()\n",
        "avg=df.select(avg(\"value\").alias(\"avg\")).show()\n",
        "#max\n",
        "mx=df.select(max(\"value\").alias(\"max\")).show()\n",
        "#min\n",
        "mn=df.select(min(\"value\").alias(\"min\")).show()\n",
        "#cnt\n",
        "mn=df.select(count(\"value\").alias(\"count\")).show()\n",
        "\n"
      ],
      "metadata": {
        "id": "34ZbDQBAxLWX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0aafa08-14ed-4894-aa1a-9156fea3902c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+\n",
            "| id|value|\n",
            "+---+-----+\n",
            "|  1|   10|\n",
            "|  2|   20|\n",
            "|  3|   30|\n",
            "|  4|   40|\n",
            "|  5|   50|\n",
            "+---+-----+\n",
            "\n",
            "number of rows in above  dataframe is: 5\n",
            "+---+\n",
            "|sum|\n",
            "+---+\n",
            "|150|\n",
            "+---+\n",
            "\n",
            "+----+\n",
            "| avg|\n",
            "+----+\n",
            "|30.0|\n",
            "+----+\n",
            "\n",
            "+---+\n",
            "|max|\n",
            "+---+\n",
            "| 50|\n",
            "+---+\n",
            "\n",
            "+---+\n",
            "|min|\n",
            "+---+\n",
            "| 10|\n",
            "+---+\n",
            "\n",
            "+-----+\n",
            "|count|\n",
            "+-----+\n",
            "|    5|\n",
            "+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''''a=int(input(\"enter first numner:\"))\n",
        "b=int(input(\"enter second numner:\"))\n",
        "sum = a+b\n",
        "print(\"sum of entered numbers is:\",sum)'''\n",
        "\n",
        "for i in range(1,11):\n",
        "    print(i)\n",
        "\n",
        "for x in (\"hello\"):\n",
        "  print(x)\n",
        "\n",
        "for x in (\"hello\",\"hi im dev\"):\n",
        "  print(x)\n",
        "\n",
        "z=[1,'dev','c']\n",
        "for c in z:\n",
        "  print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbmLMt96oiOj",
        "outputId": "667bba72-01c9-48f0-f377-4baf00f94a18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "h\n",
            "e\n",
            "l\n",
            "l\n",
            "o\n",
            "hello\n",
            "hi im dev\n",
            "1\n",
            "dev\n",
            "c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3,20,3):\n",
        "  print(i)\n",
        "\n",
        "list(range(10))\n",
        "list(range(1,10))\n",
        "list(range(-6,10))"
      ],
      "metadata": {
        "id": "aMhzVdxEzVQu",
        "outputId": "4c02cfb9-172b-434d-91fb-0431222b0ddb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "6\n",
            "9\n",
            "12\n",
            "15\n",
            "18\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numbers = [1, 2, 3, 4, 5]\n",
        "total = 0\n",
        "\n",
        "for i in numbers:\n",
        "  total = total + i\n",
        "print(total)\n",
        "\n",
        "for i in range(2,5):\n",
        "  for j in range(i):\n",
        "    print(\"*\",end=\"\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "UESyUrsUhiAB",
        "outputId": "094e6f25-3358-4d2f-b8a1-960ccfb5891a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15\n",
            "**\n",
            "***\n",
            "****\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Create a Spark session\n",
        "spark =SparkSession.builder.appName(\"dev\").getOrCreate()\n",
        "# Sample DataFrames\n",
        "emp_data = [ Row(emp_id=1, emp_name=\"Alice\", emp_salary=50000, emp_dept_id=101, emp_location=\"New York\"),\n",
        "             Row(emp_id=2, emp_name=\"Bob\", emp_salary=60000, emp_dept_id=102, emp_location=\"Los Angeles\"),\n",
        "             Row(emp_id=3, emp_name=\"Charlie\", emp_salary=55000, emp_dept_id=101, emp_location=\"Chicago\"),\n",
        "             Row(emp_id=4, emp_name=\"David\", emp_salary=70000, emp_dept_id=103, emp_location=\"San Francisco\"),\n",
        "             Row(emp_id=5, emp_name=\"Eve\", emp_salary=48000, emp_dept_id=102, emp_location=\"Houston\") ]\n",
        "dept_data = [Row(dept_id=101, dept_name=\"Engineering\", dept_head=\"John\", dept_location=\"New York\"),\n",
        "             Row(dept_id=102, dept_name=\"Marketing\", dept_head=\"Mary\", dept_location=\"Los Angeles\"),\n",
        "             Row(dept_id=103, dept_name=\"Finance\", dept_head=\"Frank\", dept_location=\"Chicago\") ]\n",
        "\n",
        "emp_columns = [\"emp_id\", \"emp_name\", \"emp_salary\", \"emp_dept_id\", \"emp_location\"]\n",
        "dept_columns = [\"dept_id\", \"dept_name\", \"dept_head\", \"dept_location\"]\n",
        "emp_df = spark.createDataFrame(emp_data, emp_columns)\n",
        "dept_df = spark.createDataFrame(dept_data, dept_columns)\n",
        "\n",
        "# Display emp data print(\"emp_data:\")\n",
        "emp_df.show()\n",
        "# Display dept data print(\"dept_data:\")\n",
        "dept_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lMqxqSIlS3Y",
        "outputId": "d02f257a-c277-4434-9653-df6ead21e7c6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+----------+-----------+-------------+\n",
            "|emp_id|emp_name|emp_salary|emp_dept_id| emp_location|\n",
            "+------+--------+----------+-----------+-------------+\n",
            "|     1|   Alice|     50000|        101|     New York|\n",
            "|     2|     Bob|     60000|        102|  Los Angeles|\n",
            "|     3| Charlie|     55000|        101|      Chicago|\n",
            "|     4|   David|     70000|        103|San Francisco|\n",
            "|     5|     Eve|     48000|        102|      Houston|\n",
            "+------+--------+----------+-----------+-------------+\n",
            "\n",
            "+-------+-----------+---------+-------------+\n",
            "|dept_id|  dept_name|dept_head|dept_location|\n",
            "+-------+-----------+---------+-------------+\n",
            "|    101|Engineering|     John|     New York|\n",
            "|    102|  Marketing|     Mary|  Los Angeles|\n",
            "|    103|    Finance|    Frank|      Chicago|\n",
            "+-------+-----------+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Join the two DataFrames based on department ID and location\n",
        "joined_df = emp_df.join(dept_df, (emp_df.emp_dept_id == dept_df.dept_id) & (emp_df.emp_location == dept_df.dept_location), \"inner\")\n",
        "\n",
        "# Select the desired columns\n",
        "result_df = joined_df.select(\n",
        "    col(\"emp_id\"),\n",
        "    col(\"emp_name\"),\n",
        "    col(\"emp_location\"),\n",
        "    col(\"dept_name\"),\n",
        "    col(\"dept_location\")\n",
        ")\n",
        "\n",
        "# Display the results\n",
        "result_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WadYgPDbo6qg",
        "outputId": "7a3b164f-8d22-453d-df52-c454b39f0280"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+------------+-----------+-------------+\n",
            "|emp_id|emp_name|emp_location|  dept_name|dept_location|\n",
            "+------+--------+------------+-----------+-------------+\n",
            "|     1|   Alice|    New York|Engineering|     New York|\n",
            "|     2|     Bob| Los Angeles|  Marketing|  Los Angeles|\n",
            "+------+--------+------------+-----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a PySpark query to get the average salary of employees\n",
        "# in each department, displaying dept_name\n",
        "# and the calculated average_salary.\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Join the two DataFrames based on department ID and location\n",
        "joined_df1 = emp_df.join(dept_df, (emp_df.emp_dept_id == dept_df.dept_id), \"inner\")\n",
        "joined_df1.show()\n",
        "\n",
        "df3=joined_df1.filter((col(\"dept_name\") == \"Engineering\") & (col(\"dept_head\") == \"John\") & (col(\"emp_name\")=='Alice'))\n",
        "df4=df3.drop(\"emp_dept_id\",\"dept_location\").withColumnRenamed(\"emp_salary\",\"Employee_salary\")\n",
        "df4.show()\n",
        "\n",
        "df2=joined_df1.groupBy(\"dept_name\").agg(avg(\"emp_salary\").alias(\"average_salary\"))\n",
        "# df2.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R35N61hvqQYA",
        "outputId": "04ad1788-d628-4a67-9583-65c1b70d7526"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+----------+-----------+-------------+-------+-----------+---------+-------------+\n",
            "|emp_id|emp_name|emp_salary|emp_dept_id| emp_location|dept_id|  dept_name|dept_head|dept_location|\n",
            "+------+--------+----------+-----------+-------------+-------+-----------+---------+-------------+\n",
            "|     1|   Alice|     50000|        101|     New York|    101|Engineering|     John|     New York|\n",
            "|     3| Charlie|     55000|        101|      Chicago|    101|Engineering|     John|     New York|\n",
            "|     2|     Bob|     60000|        102|  Los Angeles|    102|  Marketing|     Mary|  Los Angeles|\n",
            "|     5|     Eve|     48000|        102|      Houston|    102|  Marketing|     Mary|  Los Angeles|\n",
            "|     4|   David|     70000|        103|San Francisco|    103|    Finance|    Frank|      Chicago|\n",
            "+------+--------+----------+-----------+-------------+-------+-----------+---------+-------------+\n",
            "\n",
            "+------+--------+---------------+------------+-------+-----------+---------+\n",
            "|emp_id|emp_name|Employee_salary|emp_location|dept_id|  dept_name|dept_head|\n",
            "+------+--------+---------------+------------+-------+-----------+---------+\n",
            "|     1|   Alice|          50000|    New York|    101|Engineering|     John|\n",
            "+------+--------+---------------+------------+-------+-----------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Create a Spark session (if not already created)\n",
        "spark = SparkSession.builder.appName(\"dev\").getOrCreate()\n",
        "\n",
        "# Sample DataFrames\n",
        "customers_data = [\n",
        "    Row(customer_id=1, customer_name=\"John Doe\", city=\"New York\"),\n",
        "    Row(customer_id=2, customer_name=\"Jane Smith\", city=\"Los Angeles\"),\n",
        "    Row(customer_id=3, customer_name=\"David Lee\", city=\"Chicago\"),\n",
        "    Row(customer_id=4, customer_name=\"Sarah Jones\", city=\"Houston\")\n",
        "]\n",
        "orders_data = [\n",
        "    Row(order_id=101, customer_id=1, order_date=\"2023-10-26\", order_amount=100),\n",
        "    Row(order_id=102, customer_id=2, order_date=\"2023-10-26\", order_amount=200),\n",
        "    Row(order_id=103, customer_id=1, order_date=\"2023-10-27\", order_amount=150),\n",
        "    Row(order_id=104, customer_id=3, order_date=\"2023-10-27\", order_amount=300)\n",
        "]\n",
        "\n",
        "customers_columns = [\"customer_id\", \"customer_name\", \"city\"]\n",
        "orders_columns = [\"order_id\", \"customer_id\", \"order_date\", \"order_amount\"]\n",
        "customers_df = spark.createDataFrame(customers_data, customers_columns)\n",
        "orders_df = spark.createDataFrame(orders_data, orders_columns)\n",
        "\n",
        "customers_df.show()\n",
        "orders_df.show()\n",
        "\n",
        "dc1=customers_df.join(orders_df,customers_df.customer_id==orders_df.customer_id,\"inner\")\\\n",
        ".select(customers_df.customer_name,orders_df.order_date,orders_df.order_amount)\\\n",
        ".filter(col(\"order_amount\")>100)\n",
        "dc1.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nCX06MUWRFy",
        "outputId": "35f074cc-cd36-4e09-ef79-52d0e96c779b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+-----------+\n",
            "|customer_id|customer_name|       city|\n",
            "+-----------+-------------+-----------+\n",
            "|          1|     John Doe|   New York|\n",
            "|          2|   Jane Smith|Los Angeles|\n",
            "|          3|    David Lee|    Chicago|\n",
            "|          4|  Sarah Jones|    Houston|\n",
            "+-----------+-------------+-----------+\n",
            "\n",
            "+--------+-----------+----------+------------+\n",
            "|order_id|customer_id|order_date|order_amount|\n",
            "+--------+-----------+----------+------------+\n",
            "|     101|          1|2023-10-26|         100|\n",
            "|     102|          2|2023-10-26|         200|\n",
            "|     103|          1|2023-10-27|         150|\n",
            "|     104|          3|2023-10-27|         300|\n",
            "+--------+-----------+----------+------------+\n",
            "\n",
            "+-------------+----------+------------+\n",
            "|customer_name|order_date|order_amount|\n",
            "+-------------+----------+------------+\n",
            "|     John Doe|2023-10-27|         150|\n",
            "|   Jane Smith|2023-10-26|         200|\n",
            "|    David Lee|2023-10-27|         300|\n",
            "+-------------+----------+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        " # Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"EmployeeHierarchy\").getOrCreate()\n",
        " # Sample data\n",
        "data = [ (1, None, \"CEO\"), (2, 1, \"Manager A\"),\n",
        " (3, 1, \"Manager B\"), (4, 2, \"Dev\"), (5, 3, \"Anamika\"), ]\n",
        "columns = [\"empid\", \"mrgid\", \"ename\"]\n",
        "\n",
        "employee_df = spark.createDataFrame(data, columns)\n",
        "\n",
        " # Display the result\n",
        "\n",
        "print(\"emp_data:\")\n",
        "employee_df.show()\n",
        "\n",
        "\n",
        "# Self-join to find the manager and CEO\n",
        "manager_df = employee_df.alias(\"e\").join(employee_df.alias(\"m\"), col(\"e.mrgid\") == col(\"m.empid\"), \"left\")\\\n",
        ".select( col(\"e.ename\").alias(\"employee\"), col(\"m.ename\").alias(\"manager\") )\n",
        "\n",
        "# Display the result\n",
        "print(\"mgr:\")\n",
        "\n",
        "manager_df.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "p2S1A_3lyTS_",
        "outputId": "9d0c9cea-8f32-450a-9d9d-e9e1e7bdcf6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emp_data:\n",
            "+-----+-----+---------+\n",
            "|empid|mrgid|    ename|\n",
            "+-----+-----+---------+\n",
            "|    1| NULL|      CEO|\n",
            "|    2|    1|Manager A|\n",
            "|    3|    1|Manager B|\n",
            "|    4|    2|      Dev|\n",
            "|    5|    3|  Anamika|\n",
            "+-----+-----+---------+\n",
            "\n",
            "mgr:\n",
            "+---------+---------+\n",
            "| employee|  manager|\n",
            "+---------+---------+\n",
            "|      CEO|     NULL|\n",
            "|Manager A|      CEO|\n",
            "|Manager B|      CEO|\n",
            "|  Anamika|Manager B|\n",
            "|      Dev|Manager A|\n",
            "+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Create a Spark session (if not already created)\n",
        "spark = SparkSession.builder.appName(\"EmployeeHierarchy\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (1, None, \"CEO\"),\n",
        "    (2, 1, \"Manager A\"),\n",
        "    (3, 1, \"Manager B\"),\n",
        "    (4, 2, \"Dev\"),\n",
        "    (5, 3, \"Anamika\"),\n",
        "]\n",
        "columns = [\"empid\", \"mrgid\", \"ename\"]\n",
        "\n",
        "employee_df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Display the result\n",
        "print(\"emp_data:\")\n",
        "employee_df.show()\n",
        "\n",
        "# Determine hierarchy level\n",
        "# Start with CEO (level 1)\n",
        "ceo_df = employee_df.filter(col(\"mrgid\").isNull()).withColumn(\"level\", lit(1))\n",
        "\n",
        "# Iteratively add levels for other employees\n",
        "current_level = 1\n",
        "while True:\n",
        "    current_level += 1\n",
        "    next_level_df = employee_df.join(ceo_df,employee_df[\"empid\"] == ceo_df[\"mrgid\"], \"inner\")\\\n",
        "    .select(col(\"empid\"), col(\"ename\"), col(\"mrgid\"), lit(current_level).alias(\"level\"))\n",
        "\n",
        "    if next_level_df.count() == 0:\n",
        "        break\n",
        "\n",
        "    ceo_df = ceo_df.union(next_level_df)\n",
        "\n",
        "# Display the final result\n",
        "ceo_df.orderBy(\"level\", \"empid\").show()"
      ],
      "metadata": {
        "id": "07F3ADoOEj5j",
        "outputId": "1fd1bdbb-b910-4e0a-e893-c1349faf2b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "emp_data:\n",
            "+-----+-----+---------+\n",
            "|empid|mrgid|    ename|\n",
            "+-----+-----+---------+\n",
            "|    1| NULL|      CEO|\n",
            "|    2|    1|Manager A|\n",
            "|    3|    1|Manager B|\n",
            "|    4|    2|      Dev|\n",
            "|    5|    3|  Anamika|\n",
            "+-----+-----+---------+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "Column empid#1530L are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-96de42a15117>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mcurrent_level\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mnext_level_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0memployee_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mceo_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0memployee_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"empid\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mceo_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mrgid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"empid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ename\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mrgid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"level\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how)\u001b[0m\n\u001b[1;32m   2491\u001b[0m                 \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2492\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"how should be a string\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2493\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2494\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Column empid#1530L are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check."
          ]
        }
      ]
    }
  ]
}